{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "from selenium import webdriver\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://www.indeed.com/viewjob?jk=018ed0ca7a7ce1a2&tk=1deup3s155ov5804&from=serp&vjs=3\"\n",
    "driver = webdriver.Chrome(r'C:/Users/trang/Desktop/bootcamp/bootcampNYCDSA/Python/scrape_indeed/chromedriver_win32/chromedriver.exe')\n",
    "driver.get(url)\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "driver.close()\n",
    "posting = soup.find(name = \"div\", attrs = {\"class\":\"jobsearch-JobComponent-description\"}).text\n",
    "posting = posting.lower()\n",
    "posting\n",
    "type(posting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the page html\n",
    "#parameter link to scrape\n",
    "#output: beautiful soup object of html source\n",
    "def get_soup(url):\n",
    "    driver = webdriver.Chrome(r'C:/Users/trang/Desktop/bootcamp/bootcampNYCDSA/Python/scrape_indeed/chromedriver_win32/chromedriver.exe')\n",
    "    driver.get(url)\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    driver.close()\n",
    "    return soup\n",
    "\n",
    "#getting link from the page source\n",
    "#parameter: soup object\n",
    "#output: the urls of job postings that need to be scraped\n",
    "def get_joblinks(soup):\n",
    "    urls = []\n",
    "    \n",
    "    #loop through all the posting links\n",
    "    for link in soup.find_all('a', {'class': 'jobtitle'}):\n",
    "                \n",
    "        partial_url = link.get('href')\n",
    "        \n",
    "        #filter out sponsored posting\n",
    "        if \"www.indeed.com\" in partial_url:\n",
    "            url =  partial_url \n",
    "        elif '/pagead/' not in partial_url:\n",
    "            url = \"http://www.indeed.com\" + partial_url\n",
    "            urls.append(url)\n",
    "    return urls\n",
    "\n",
    "#get job links of a specified search with job title, number of page and location\n",
    "#output:all of the urls of a certain number of page according to title and location\n",
    "def get_search(title, num_page, location):\n",
    "    \n",
    "    #link generator\n",
    "    base_url = 'https://www.indeed.com/jobs?q={}&l={}'.format(title, location)\n",
    "    soup = get_soup(base_url)\n",
    "    urls = get_joblinks(soup)\n",
    "    #print(len(urls))\n",
    "    \n",
    "    #number of job postings in 1 search:\n",
    "    posting_strcount = soup.find(name = \"div\", attrs = {\"id\" : \"searchCount\"}).text.strip()\n",
    "    posting_strcount = posting_strcount[posting_strcount.find('of')+2:-4].strip()\n",
    "   \n",
    "    try:\n",
    "        posting_count = int(posting_strcount)\n",
    "    except ValueError:\n",
    "        posting_count = int(posting_strcount.replace(',',''))\n",
    "        pass\n",
    "    #print(posting_count)\n",
    "    \n",
    "    \n",
    "    #scrpae all of the pages\n",
    "    for i in range(2, num_page +1):\n",
    "        start_num = (i-1)*10\n",
    "        base_url = \"https://www.indeed.com/jobs?q={}&l={}&start={}\".format(title, location,start_num)\n",
    "        try:\n",
    "            soup = get_soup(base_url)\n",
    "            urls += get_joblinks(soup)\n",
    "        except:\n",
    "            continue\n",
    "    return urls\n",
    "    #print(urls)\n",
    "\n",
    "    \n",
    "###get text from each job posting\n",
    "#parameter: url of a job posting\n",
    "#output: return title, company name, and job description\n",
    "def get_text(url):\n",
    "    soup = get_soup(url)\n",
    "    title = soup.find(name = 'h3').text\n",
    "    company_name = soup.find(name = \"h4\", attrs ={ \"class\":\"jobsearch-CompanyReview--heading\"}).text\n",
    "    posting = soup.find(name = \"div\", attrs = {\"class\":\"jobsearch-JobComponent-description\"}).text\n",
    "    posting = posting.lower()\n",
    "    \n",
    "    return title, company_name, posting\n",
    "\n",
    "\n",
    "##get data:get all of the job posting data and save in a json file\n",
    "#parameter: title number of page and location\n",
    "#output: get all data needed and save in json file\n",
    "def get_data(j_title, num_page, location):\n",
    "    \n",
    "    #convert to to indeed format\n",
    "    j_title = '+'.join(j_title.lower().split())\n",
    "    \n",
    "    jobs_dict = {}\n",
    "    \n",
    "    #get the urls for the parameters\n",
    "    urls = get_search(j_title, num_page, location)\n",
    "    \n",
    "    #create the nested dictionary\n",
    "    num_urls = len(urls)\n",
    "    for i, url in enumerate(urls):\n",
    "        try:\n",
    "            title,company_name ,posting = get_text(url)\n",
    "            jobs_dict[i] = {} #create nested dictionary\n",
    "            jobs_dict[i]['title'] = title\n",
    "            jobs_dict[i]['company name'] = company_name\n",
    "            jobs_dict[i]['posting'] = posting\n",
    "            jobs_dict[i]['url'] = url\n",
    "            \n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "        percent = (i+1) / num_urls\n",
    "        print(\"Progress: {:2.0f}%\".format(100*percent), end='\\r')\n",
    "    \n",
    "    #save the dictionary as json file:\n",
    "    file_name = j_title.replace('+','_') + '_'+location.replace(' ','_') + '.json'\n",
    "    with open(file_name, 'w') as f:\n",
    "        json.dump(jobs_dict, f)\n",
    "        \n",
    "    print('All {} postings have been scraped and saved!'.format(num_urls))\n",
    "    \n",
    "        \n",
    "#get_data('data scientist', 10,'Seatle')   \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cities = ['New York city', 'Boston','Austin', 'Chicago', 'Philadelphia', 'San Diego',\n",
    "          'San Francisco', 'Seattle', 'Washington DC','Portland']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All 100 postings have been scraped and saved!\n",
      "All 100 postings have been scraped and saved!\n",
      "All 99 postings have been scraped and saved!\n"
     ]
    }
   ],
   "source": [
    "cities = [ 'Seattle', 'Washington DC','Portland']\n",
    "for i in cities:\n",
    "    get_data('data analyst', 10, i)\n",
    "    #get_data('data scientist',10,i)\n",
    "    #get_data('data engineer', 10, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
